# VLM-Adversarial-Attacks
## Installation

### 1. Set up the environment

```sh
git clone https://github.com/yxc0731/VLM-Adversarial-Attacks.git

cd VLM-Adversarial-Attacks

pip install -r requirements.txt
```
### 2. Generate Visual Adversarial Examples
```sh
python clip_attack.py
```

## Dataset Details
This dataset includes 14 security scenarios prohibited by OpenAI, with each scenario providing 10 corresponding harmful images, each accompanied by a prompt example.
## Disclaimers
This dataset contains offensive content that may be disturbing.
