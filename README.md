# VLM-Adversarial-Attacks
## Installation

We take MiniGPT-4 (13B) as the sandbox to showcase our attacks. The following installation instructions are adapted from the MiniGPT-4 repository.

### 1. Set up the environment

```sh
git clone https://github.com/yourusername/VLM-Adversarial-Attacks.git

cd VLM-Adversarial-Attacks

pip install -r requirements.txt

## Generate Visual Adversarial Examples

## Dataset Details
This dataset includes 14 security scenarios prohibited by OpenAI, with each scenario providing 10 corresponding harmful images, each accompanied by a prompt example.
## Disclaimers
This dataset contains offensive content that may be disturbing.
